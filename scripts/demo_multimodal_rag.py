import asyncio
import json
import logging
import os
from pathlib import Path

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… Environment variables loaded")
except ImportError:
    print("âš ï¸ python-dotenv not installed")

# RAGAnything imports
try:
    from raganything import RAGAnything
    from lightrag.llm.openai import openai_complete_if_cache, openai_embed
    from lightrag import LightRAG
    RAG_ANYTHING_AVAILABLE = True
except ImportError:
    RAG_ANYTHING_AVAILABLE = False
    print("âš ï¸ RAGAnything not available - install with: pip install raganything")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def demo_knowledge_graph():
    """Demonstrate the knowledge graph and multimodal capabilities"""
    
    print("\n" + "="*80)
    print("ğŸš€ RAG-ANYTHING MULTIMODAL DEMO")
    print("="*80)
    print("ğŸ“Š Knowledge Graph & Multimodal Content Processing")
    print("="*80)
    
    if not RAG_ANYTHING_AVAILABLE:
        print("âŒ RAGAnything not available - please install with: pip install raganything")
        return
    
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âŒ OPENAI_API_KEY not found in environment")
        return
    
    try:
        # Initialize RAGAnything
        print("ğŸ”§ Initializing RAG-Anything...")
        
        def embedding_func(texts):
            return openai_embed(
                texts,
                model="text-embedding-3-large", 
                api_key=openai_api_key,
            )
        embedding_func.embedding_dim = 3072
        
        working_dir = "./rag_storage"
        Path(working_dir).mkdir(exist_ok=True)
        
        lightrag_instance = LightRAG(
            working_dir=working_dir,
            llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=openai_api_key,
                **kwargs,
            ),
            embedding_func=embedding_func
        )
        
        rag = RAGAnything(lightrag=lightrag_instance)
        print("âœ… RAG-Anything initialized successfully!")
        
        # Demonstrate current knowledge graph
        print("\nğŸ“Š KNOWLEDGE GRAPH STATUS:")
        print("-" * 40)
        
        # Check vector database files
        vdb_files = list(Path(working_dir).glob("*.json"))
        print(f"Vector files: {len(vdb_files)}")
        for f in vdb_files:
            size = f.stat().st_size
            print(f"  ğŸ“„ {f.name}: {size} bytes")
        
        # Check processed content
        processed_dir = Path("rag_output")
        if processed_dir.exists():
            content_files = list(processed_dir.rglob("*_content_list.json"))
            if content_files:
                with open(content_files[0], 'r', encoding='utf-8') as f:
                    content_list = json.load(f)
                    print(f"Content items: {len(content_list)}")
                    
                    # Count content types
                    text_count = sum(1 for item in content_list if item.get("type") == "text")
                    image_count = sum(1 for item in content_list if item.get("type") == "image")
                    table_count = sum(1 for item in content_list if item.get("type") == "table")
                    
                    print(f"  ğŸ“ Text blocks: {text_count}")
                    print(f"  ğŸ–¼ï¸ Images: {image_count}")
                    print(f"  ğŸ“‹ Tables: {table_count}")
        
        # Demonstrate content analysis
        print("\nğŸ” CONTENT ANALYSIS:")
        print("-" * 40)
        
        # Look for processed markdown
        md_files = list(processed_dir.rglob("*.md"))
        if md_files:
            print(f"ğŸ“„ Markdown files: {len(md_files)}")
            with open(md_files[0], 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"  Content length: {len(content)} characters")
                
                # Extract key information
                if "PT1 series" in content:
                    print("  ğŸ¯ Found: PT1 Draw-wire Sensors")
                if "TECHNICAL DATASHEETS" in content:
                    print("  ğŸ“‹ Found: Technical datasheets")
                if "Key features" in content:
                    print("  âœ¨ Found: Key features section")
        
        # Demonstrate simple query capability
        print("\nğŸ¤– KNOWLEDGE GRAPH QUERIES:")
        print("-" * 40)
        
        # Add some content if knowledge graph is empty
        if not vdb_files or all(f.stat().st_size == 0 for f in vdb_files):
            print("ğŸ“š Adding content to knowledge graph...")
            
            # Use the processed content
            if md_files:
                await rag.process_document_complete(
                    file_path=str(md_files[0]),
                    output_dir="./rag_output_kg",
                    parse_method="auto"
                )
                print("âœ… Content added to knowledge graph!")
        
        # Try some queries
        test_queries = [
            "What are the main features of PT1 sensors?",
            "What applications are PT1 sensors used for?",
            "What technical datasheets are available?"
        ]
        
        for query in test_queries:
            try:
                print(f"\nâ“ Query: {query}")
                result = await lightrag_instance.aquery(query, param="hybrid")
                print(f"ğŸ’¡ Answer: {result[:200]}..." if len(result) > 200 else f"ğŸ’¡ Answer: {result}")
            except Exception as e:
                print(f"âŒ Query failed: {e}")
        
        # Demonstrate multimodal capabilities
        print("\nğŸ¨ MULTIMODAL CAPABILITIES:")
        print("-" * 40)
        print("ğŸ–¼ï¸ Image Processing: Extract and analyze images from PDFs")
        print("ğŸ“‹ Table Processing: Parse and understand tabular data")
        print("ğŸ“„ Text Processing: Extract and chunk text content")
        print("ğŸ”— Relationship Mapping: Create knowledge graph connections")
        
        print("\nğŸ’¡ RAG-Anything Features demonstrated:")
        print("  âœ… Knowledge graph creation")
        print("  âœ… Vector embeddings (3072-dimensional)")
        print("  âœ… Content extraction and processing")
        print("  âœ… Multimodal content support")
        print("  âœ… Semantic search capabilities")
        
        print("\nğŸš€ Usage Examples:")
        print("  python scripts/rag_chat_interface.py chat")
        print("  python scripts/rag_chat_interface.py query 'your question'")
        print("  python scripts/rag_chat_interface.py explore")
        print("  python scripts/rag_chat_interface.py process path/to/pdf")
        
    except Exception as e:
        logger.error(f"âŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(demo_knowledge_graph())